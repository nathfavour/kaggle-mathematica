{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":73231,"databundleVersionId":8365361,"sourceType":"competition"}],"dockerImageVersionId":30698,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"This notebook demonstrates the use of the evaluation API for the AIMO competition. It has the following features:\n- Test problems are served to your model one-at-a-time and in random order. The random ordering will vary across submissions.\n- Each problem will only be served once. You must make a prediction for each problem before being served the next problem. You cannot change previously made predictions.","metadata":{}},{"cell_type":"code","source":"# Define your model\nclass Model:\n    def predict(self, x):\n        return 0\n\n# What am I doing?\nmodel = Model()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set up the evaluation API\nimport aimo\n\nenv = aimo.make_env()\niter_test = env.iter_test()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Note that this public version of the API does not randomize the order in which problems are served. The API used when your submission is scored will randomize the order.","metadata":{}},{"cell_type":"code","source":"# Iterate through the test set and use the model make predictions\nfor test, sample_submission in iter_test:\n    sample_submission['answer'] = model.predict(test['problem'])\n    env.predict(sample_submission)\n    print(test)\n    print(sample_submission, '\\n')","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}